---
layout:     post
title:      "图形渲染管线"
subtitle:   "The Graphics Rendering Pipeline"
date:       2017-09-19 00:45:00
author:     "MBB"
header-img: "img/in-post/real-time-rendering/the-graphics-rendering-pipeline/background.png"
header-mask: 0.3
catalog:    true
tags:
    - 图形学
    - 实时渲染
    - 读书笔记
---
>   "A chain is no stronger than its weakest link."
>   -Anonymous

## 管线结构
实时渲染管线可以粗略地划分成三个*概念阶段*——**应用阶段**、**几何阶段**、**光栅化阶段**。这种结构是实时渲染管线的核心，也是后面探讨内容的基础。每一个阶段内部通常也是一条管线，也就是说这些阶段由几个子阶段构成。我们需要区分**概念阶段**、**功能阶段**和**管线阶段**这三种不同类型的阶段。一个**功能阶段**有具体的任务执行，但是并不会具体指定任务在管线中的执行方式。另一方面，一个**管线阶段**又与其他所有的**管线阶段**同步执行。一个**管线阶段**也可能出于性能的考虑而被并行化。

**应用阶段**是由应用程序驱动的，在CPU中执行。我们知道，CPU通常由多核组成，擅长并行执行多个线程，这就使得CPU能够高效地执行大量的任务，这也是**应用阶段**负责做的工作。通常在CPU上完成的工作主要包括碰撞检测，全局加速算法，动画，物理模拟等等，这些都取决于应用的类型。

**几何阶段**主要处理几何变换、投影变换等，这个阶段通过计算决定什么需要被绘制，怎样绘制以及绘制到什么地方。**几何阶段**主要是在GPU上进行的。

**光栅化阶段**根据上一阶段产生的数据以及期望的逐像素计算来绘制(渲染)最终的图像。**光栅化阶段**完全在GPU上进行。

## 应用阶段
这个阶段完全由开发者掌控，因此可以先进行功能实现后再持续优化。这个阶段的优化可能影响后续阶段，比如通过算法或设置减少需要渲染的三角形数量。

在**应用阶段**的最后，需要被渲染的几何体(图元，例如点、线、三角形等)被传递到了**几何阶段**，为了效率，**应用阶段**都会尽量并行化。

该阶段通常处理碰撞检测、设备输入、纹理动画、动画以及一些加速算法等。

## 几何阶段
**几何阶段**主要进行逐多边形和逐顶点操作。该阶段可以进一步划分为以下几个功能阶段:**模型视图变换**、**顶点着色**、**投影**、**裁剪**以及**屏幕映射**。再次强调，根据具体的实现，这些**功能阶段**可能等同于或者不等同于**管线阶段**。比如在某些情况下，数个连续**功能阶段**组成一个**管线阶段**(与其他**管线阶段**并行执行)。在其他一些情况下，一个**功能阶段**被划分成几个更小的**管线阶段**。
- **模型视图变换**：坐标变换，从模型空间变换到世界空间再变换到视觉空间。这些变换都通过4x4的矩阵进行运算。

- **顶点着色**：根据材质、顶点属性、光照等因素计算出各种数据(如颜色、各种向量、纹理坐标等)，然后再传递到**光栅化阶段**进行插值处理。着色计算根据不同的计算类型会在不同的空间中进行，比如在根据光照模型进行光照计算时，我们一般都在世界坐标下进行计算，而使用shadowmap生成阴影时，我们会在光源空间下生成该光源的shadowmap。

- **投影**：将[视体][0]变换到坐标范围为`(-1,-1,-1)`到`(1,1,1)`的规范化视体中。投影方式通常分为透视投影与正交投影。投影变换可以通过一个4x4的矩阵来进行，投影变换过后的坐标我们称之为归一化设备坐标(NDC)。投影变换之后的Z坐标并没有存储在我们生成的图像中，而是存储到了Z-Buffer中。

- **裁剪**：全部在视体内的图元传递到下一阶段，全部在视体外的图元则被丢弃不渲染，部分在内的进行裁剪。裁剪通常是由固定功能硬件完成的。

- **屏幕映射**：从NDC坐标映射到<code>(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>)(x<sub>1</sub><x<sub>2</sub>,y<sub>1</sub><y<sub>2</sub>)</code>，z坐标保持不变传递到**光栅化阶段**。将浮点数坐标映射到整型像素坐标，DX9及之前将`0.0`当做像素中心，因此点像素范围`[0,9]`涵盖的浮点坐标范围为`[-0.5,9.5]`；OpenGL和DX10及其之后将`0.5`当作像素中心，因此像素范围`[0.9]`涵盖的浮点坐标为`[0.0,10.0)`。OpenGL以左下角为坐标原点，DX以左上角为坐标原点。

## 光栅化阶段
根据屏幕中的各个点附带的各种信息(深度值、着色数据等)，将这些点转换为屏幕上的像素。该阶段也分为以下几个功能阶段。
- **三角形建立**：根据三角形表面计算出其他数据。这些数据将提供给扫描线转换算法进行扫描线转换以及对**几何阶段**产生的各种着色数据进行插值。该阶段是由固定功能硬件完成的。

- **三角形遍历**：找出三角形所覆盖的像素并为这些像素生成片元。片元属性由三角形的三个顶点属性插值得到。这些属性包括片元深度以及**几何阶段**得到的着色数据。

- **像素着色**：以插值得到的着色数据作为输入的逐像素着色都在这个阶段进行。该阶段是完全可编程的，大量的着色技术应用于该阶段，开发者可以在该阶段通过编程实现他们想要的效果。

- **合并**：通过Z-Buffer中的Z值大小来决定渲染哪个图元，Z值越小离相机越近的图元被渲染，但是必须在所有不透明物体被渲染完之后再进行半透明物体的渲染，这也是Z-Buffer的缺点之一。Color Buffer中还有一个alpha通道，在片元进行深度测试前还可以进行一次透明测试，该测试与一个特定值比较(小于，大于，等于，等等)，若测试不通过则丢弃该片元，不再进行后续处理。该测试主要保证透明物体不会影响Z-Buffer。在透明度测试之后，深度测试之前，可以使用[Stencil Buffer][1]进行一次模板测试来实现特定效果。而[Frame Buffer][2]则可以用于实现多种屏幕后处理特效以及反走样、软阴影等。

## 总结
本文中所说的渲染管线是由从几十种专门用于*实时渲染*的图形API以及图形硬件的发展总结而来。需要注意的是，这种渲染管线并不是唯一的，离线渲染也在不同的进化路线上发展着。从早期的固定功能管线到如今的可编程管线，图形硬件以及图形API为编程者提供了更加灵活的方式来实现各种酷炫的效果，而这种可编程管线的方式使得GPU的特性得到充分利用。

[0]:https://msdn.microsoft.com/en-us/library/ff634570.aspx
[1]:https://en.wikibooks.org/wiki/OpenGL_Programming/Stencil_buffer
[2]:http://www.songho.ca/opengl/gl_fbo.html